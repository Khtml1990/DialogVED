# mode can be either CLUSTER or STANDALONE or LOCAL
PREFIX=/home/v-bolunyao/wchen2

DATA_DIR=${PREFIX}/finetune/personachat

# dailydialog fine-tune and evaluation
# this script is for fine-tune and evaluation for dailydialog corpus
########################################################################################################################
PROJECT_PATH='.'
USER_DIR=${PROJECT_PATH}/DialogVPT
NUM_WORKERS=20

# step 1: pre-process
########################################################################################################################

# step 2: fine-tune -> continue training
########################################################################################################################
SUFFIX='_vae_test_only_latent_no_attention'
SAVE_DIR=${DATA_DIR}/checkpoints/${SUFFIX}
TENSORBOARD_LOGDIR=${DATA_DIR}/tensorboard/${SUFFIX}
BINARY_DIR=${DATA_DIR}/binary/
#PRETRAINED_MODEL=${PREFIX}/checkpoints/vae_checkpoint1.pt
#PRETRAINED_MODEL=${PREFIX}/checkpoints/vae_checkpoint1.pt
#PRETRAINED_MODEL=${PREFIX}/checkpoints/checkpoint5.pt
PRETRAINED_MODEL=/mnt/my_outputs/wchen2/checkpoints/_vae_bow_lm_thanos_1800_16/checkpoint3.pt
PRETRAINED_MODEL=/mnt/my_outputs/wchen2/checkpoints/_vae_bow_lm_standard_1800_16/checkpoint1.pt


ARCH=ngram_transformer_prophet_vae_standard
CRITERION=ngram_language_loss
TASK=seq2seq_vae
#TASK=translation_prophetnet

# lr 0.0001 0.0002 0.0003
# label-smoothing 0.05/0.1/0.15/0.2
# dropout/attention-dropout/weight-decay 0.1/0.1 0.01

"$(which fairseq-train)" \
  ${BINARY_DIR} \
  --fp16 \
  --user-dir ${USER_DIR} --task ${TASK} --arch ${ARCH} \
  --optimizer adam --adam-betas '(0.9, 0.98)' --adam-eps 1e-6 --clip-norm 1.0 \
  --lr 0.0002 --lr-scheduler inverse_sqrt --min-lr 1e-09 \
  --warmup-init-lr 1e-07 --warmup-updates 500 \
  --criterion $CRITERION \
  --update-freq 4 --max-tokens 4500 --max-sentences 16 \
  --num-workers ${NUM_WORKERS}  \
  --dropout 0.1 \
  --attention-dropout 0.1 \
  --activation-dropout 0.0 \
  --weight-decay 0.01 \
  --label-smoothing 0.1 \
  --encoder-layer-drop 0.0 \
  --save-dir ${SAVE_DIR} \
  --max-epoch 10 \
  --keep-last-epochs 10 \
  --max-source-positions 512 \
  --max-target-positions 128 \
  --kl-loss-weight 0.0 \
  --cls-bow-loss-weight 0.0 \
  --latent-bow-loss-weight 1.0 \
  --masked-lm-loss-weight 0.0 \
  --tensorboard-logdir ${TENSORBOARD_LOGDIR} \
  --dataset-impl mmap \
  --empty-cache-freq 64 \
  --seed 1 \
  --skip-invalid-size-inputs-valid-test \
  --distributed-no-spawn \
  --ddp-backend no_c10d \
  --load-from-pretrained-model ${PRETRAINED_MODEL} \
  --add-cls-to-source
#  --masked-source \

# step 2: inference on test set
########################################################################################################################
SUFFIX='_vae_test_only_latent_beam_1'
BEAM_SIZE=1
#CHECK_POINT=${SAVE_DIR}/checkpoint_best.pt
CHECK_POINT=${SAVE_DIR}/checkpoint3.pt
#TASK=seq2seq_vae
TASK=translation_prophetnet

# predicted response is generated by `fairseq-generate` command
UNSORTED_OUTPUT_FILE=${DATA_DIR}/unsorted${SUFFIX}.txt
SORTED_OUTPUT_FILE=${DATA_DIR}/sorted${SUFFIX}.txt

# beam search
"$(which fairseq-generate)" \
  ${DATA_DIR}/binary \
  --path ${CHECK_POINT} \
  --user-dir ${USER_DIR} \
  --task ${TASK} \
  --batch-size 64 \
  --gen-subset test \
  --num-workers 4 \
  --no-repeat-ngram-size 3 \
  --lenpen 1 \
  --beam ${BEAM_SIZE} \
  2>&1 >"${UNSORTED_OUTPUT_FILE}"


#"$(which fairseq-generate)" \
#  ${DATA_DIR}/binary \
#  --path ${CHECK_POINT} \
#  --user-dir ${USER_DIR} \
#  --task ${TASK} \
#  --batch-size 64 \
#  --gen-subset test \
#  --num-workers 4 \
#  --no-repeat-ngram-size 3 \
#  --lenpen 1 \
#  --beam ${BEAM_SIZE} \
#  --deterministic \
#  2>&1 >"${UNSORTED_OUTPUT_FILE}"

## diverse beam search
#"$(which fairseq-generate)" \
#  ${DATA_DIR}/binary \
#  --path ${CHECK_POINT} \
#  --user-dir ${USER_DIR} \
#  --task ${TASK} \
#  --batch-size 32 \
#  --gen-subset test \
#  --num-workers 4 \
#  --no-repeat-ngram-size 3 \
#  --lenpen 1 \
#  --beam 16 \
#  --diverse-beam-groups 4 \
#  --diverse-beam-strength 0.5 \
#  2>&1 >"${UNSORTED_OUTPUT_FILE}"
#
#
## sampling
#"$(which fairseq-generate)" \
#  ${DATA_DIR}/binary \
#  --path ${CHECK_POINT} \
#  --user-dir ${USER_DIR} \
#  --task ${TASK} \
#  --batch-size 128 \
#  --gen-subset test \
#  --num-workers 4 \
#  --no-repeat-ngram-size 3 \
#  --lenpen 1 \
#  --sampling \
#  --sampling-topk 100 \
#  --nbest 1 \
#  --beam 1 \
#  2>&1 >"${UNSORTED_OUTPUT_FILE}"

# interactive
"$(which fairseq-interactive)" \
  ${DATA_DIR}/binary \
  --path ${CHECK_POINT} \
  --user-dir ${USER_DIR} \
  --task ${TASK} \
  --beam 1


# resort output
grep ^H "${UNSORTED_OUTPUT_FILE}" | cut -c 3- | sort -n | cut -f3- | sed "s/ ##//g" > ${SORTED_OUTPUT_FILE}


# step 3: evaluation with gold references
#################################################################################################
EVALUATE_LOG_PATH=${DATA_DIR}/result${SUFFIX}.txt

if [ -f ${EVALUATE_LOG_PATH} ]; then
  rm ${EVALUATE_LOG_PATH}
fi

python utils/evaluate.py \
  -name dailydialog \
  -hyp ${SORTED_OUTPUT_FILE} \
  -ref ${DATA_DIR}/processed/test.tgt \
  -out ${EVALUATE_LOG_PATH}
